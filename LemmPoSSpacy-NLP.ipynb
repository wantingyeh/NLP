{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4921d4dc-ff1a-4c41-a622-6e2a0e151c24",
   "metadata": {},
   "source": [
    "# Readme\n",
    "\n",
    "date: 08-11-2022 \\\n",
    "written by: Wan-Ting Yeh\n",
    "\n",
    "## purpose of the script:\n",
    "\n",
    "1. Walk through all the txt files in one folder\n",
    "2. use Spacy (NLP) to:\n",
    "    - tokenise the text \n",
    "    - clean the data (Exclude unwanted token, eg., punctuation)\n",
    "    - lemmentisation (talked, talking --> talk)\n",
    "    - custominsed lemmentisation (eg., peeeeeekaboo --> peekaboo)\n",
    "    - count unique word / total word / type-token ratio\n",
    "    - list part of word (noun, pronoun, adj...)\n",
    "3. ouput file\n",
    "    - OUTPUT_PATH_final: unique word\n",
    "        - filename, unique words, total numbers of a file, total unique word counts, type-token ratio\n",
    "    - OUTPUT_PATH_pos: part of speech\n",
    "         - filename, original word, word lema, part of speech, explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d16b19-b2f9-4bfa-a62e-4c2bd3dff37c",
   "metadata": {},
   "source": [
    "### Step1: load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b1f5f6f-c770-4157-81e0-31d5e2b2ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f012c-ec49-431b-9811-548a534c0450",
   "metadata": {},
   "source": [
    "### Step2: Specify constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf13388e-6dbb-4c47-bc7e-2fe5b8901241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "FOLDER = \"C:/Users/USER/PycharmProjects/UniqueWordCalculator/text_agent_condition/parent/test/\"\n",
    "OUTPUT_PATH_final = FOLDER + \"/uniques_parent_spacy_unique_test.csv\"\n",
    "OUTPUT_PATH_pos = FOLDER + \"/uniques_parent_spacy_pos.csv\"\n",
    "\n",
    "# final df output file format (count: unique words and total number words)\n",
    "columns_final = ['filename', 'unique_words', 'total_number_words', 'unique_word_count', 'TTR']\n",
    "final_df = pd.DataFrame(columns=columns_final)\n",
    "\n",
    "# part of speech output file format\n",
    "columns_pos = ['filename, word, word_lemma, word_pos, pos_explain']\n",
    "pos_df = pd.DataFrame(columns=columns_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09a56db0-dd50-419a-8f5d-46b6ff150683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of txt files in the directory\n",
    "\n",
    "def list_text_files(folder, extension=\".txt\"):\n",
    "    text_file_paths = []\n",
    "    for root, dir, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(extension):\n",
    "                text_file_paths.append(os.path.join(root, file))\n",
    "    return text_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1fd91da-336a-4f64-b3fe-77c68d4eb9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path ['C:/Users/USER/PycharmProjects/UniqueWordCalculator/text_agent_condition/parent/test/DIME002-CV.txt', 'C:/Users/USER/PycharmProjects/UniqueWordCalculator/text_agent_condition/parent/test/DIME002-DG.txt', 'C:/Users/USER/PycharmProjects/UniqueWordCalculator/text_agent_condition/parent/test/DIME004-CV.txt']\n"
     ]
    }
   ],
   "source": [
    "# check the files\n",
    "\n",
    "text_file_paths = list_text_files(FOLDER)\n",
    "print(f\"path {text_file_paths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1003a4-98f1-4219-b984-031bedcfac2b",
   "metadata": {},
   "source": [
    "### Customised the lemmentisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61560513-7cd6-4c3f-9672-bf4f87cbc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = nlp.get_pipe(\"attribute_ruler\")\n",
    "#syntax: ar.add([[{\"TEXT\":\"Bro\"}], [{\"TEXT\": \"Brah\"}]],{\"LEMMA\":\"brother\"})\n",
    "\n",
    "ar.add([[{\"TEXT\": \"cant\"}]],{\"LEMMA\":\"can't\"})\n",
    "ar.add([[{\"TEXT\":\"aaahhh\"}], [{\"TEXT\": \"aaahh\"}], [{\"TEXT\": \"aaah\"}], [{\"TEXT\": \"ahhhh\"}], [{\"TEXT\": \"ahhh\"}], [{\"TEXT\": \"ahh\"}], [{\"TEXT\": \"aah\"}], [{\"TEXT\": \"aaaa\"}], [{\"TEXT\": \"aaa\"}]], {\"LEMMA\":\"ah\"})\n",
    "ar.add([[{\"TEXT\":\"awhhhh\"}], [{\"TEXT\": \"awwww\"}], [{\"TEXT\": \"aww\"}], [{\"TEXT\": \"awh\"}]],{\"LEMMA\":\"aw\"})\n",
    "ar.add([[{\"TEXT\":\"bahhh\"}], [{\"TEXT\":\"bahh\"}]], {\"LEMMA\":\"bah\"})\n",
    "ar.add([[{\"TEXT\":\"boooeeehh\"}], [{\"TEXT\": \"booooo\"}], [{\"TEXT\": \"boooo\"}], [{\"TEXT\": \"booo\"}]],{\"LEMMA\":\"boo\"})\n",
    "ar.add([[{\"TEXT\":\"blahalalala\"}]],{\"LEMMA\":\"blabla\"})\n",
    "ar.add([[{\"TEXT\":\"bluey\"}]],{\"LEMMA\":\"blue\"})\n",
    "ar.add([[{\"TEXT\":\"baba\"}]],{\"LEMMA\":\"bubba\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a10a2-c889-486d-8897-ee9311fa4e4a",
   "metadata": {},
   "source": [
    "### Loop through file\n",
    "\n",
    "1. tokenise the data\n",
    "2. lemmentise data with customised words\n",
    "3. clean the data (no punctuation, space and \\n)\n",
    "4. **Exclude data**\n",
    "    - interjection (eg., wow, whoa, yay)\n",
    "    - punctuation\n",
    "    - space\n",
    "    - X (other)\n",
    "    - symbol\n",
    "    - manual: manual_exlcude\n",
    "5. print out [word, word.lemma, word part of speech, explain part of speech]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2273ae39-7441-4b3a-a38c-ed1e49f5f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually exclude words\n",
    "manual_exclude = [\"laugh\", \"gasp\",\" \"]\n",
    "\n",
    "\n",
    "\n",
    "for file_path in text_file_paths:\n",
    "  with open(file_path, mode='r', encoding='cp1252') as text_file:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    text = text_file.read()\n",
    "\n",
    "    #cleaning texts\n",
    "    text = text.lower()\n",
    "    text = nlp(text)\n",
    "    \n",
    "    filter_token = []\n",
    "    token_pos = []\n",
    "    unique = []\n",
    "    lines = []\n",
    "    \n",
    "    for word in text:\n",
    "        if word.pos_ not in [\"INTJ\", \"SPACE\", \"PUNCT\", \"X\", \"SYM\"] and word.lemma_ not in manual_exclude:\n",
    "            filter_token.append(word.lemma_)\n",
    "            token_pos.append(word.pos_)\n",
    "            line = [file_name, word, word.lemma_, word.pos_, spacy.explain(word.pos_)]\n",
    "            lines.append(line)\n",
    "            for token in filter_token:\n",
    "                if token not in unique:\n",
    "                    unique.append(token)\n",
    "                    \n",
    "        # numbers of words/ unique words\n",
    "    total_num = len(text)\n",
    "    total_unique = len(unique)\n",
    "    TTR = total_unique/total_num\n",
    "    \n",
    "    # save output for final df\n",
    "    data_row = pd.DataFrame(dict(zip(columns_final, [file_name, [unique], total_num, total_unique, TTR])))\n",
    "    final_df = pd.concat([final_df, data_row], axis=0, ignore_index=True) \n",
    "    \n",
    "    # save output for part of speech\n",
    "    data_row_pos = pd.DataFrame(dict(zip(columns_pos, [lines])))\n",
    "    pos_df  = pd.concat([pos_df, data_row_pos], axis=0, ignore_index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00249f1-a88f-43dd-8260-c80c8f2109d7",
   "metadata": {},
   "source": [
    "## save files\n",
    "\n",
    "1. final_df = unique word file\n",
    "    - filename, unique words, total numbers of a file, total unique word counts\n",
    "2. pos_df = part of speech file\n",
    "    - ['filename', 'original_word', 'word_lemma', 'word_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fedba85-c283-4f13-87b8-ab26ab593584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>total_number_words</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>TTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DIME002-CV.txt</td>\n",
       "      <td>[it,  is, go, play, peekaboo, this, be, we, to...</td>\n",
       "      <td>157</td>\n",
       "      <td>29</td>\n",
       "      <td>0.184713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DIME002-DG.txt</td>\n",
       "      <td>[it,  is, change, do, you, want, to, play, pee...</td>\n",
       "      <td>150</td>\n",
       "      <td>34</td>\n",
       "      <td>0.226667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DIME004-CV.txt</td>\n",
       "      <td>[you, be, like, a, magician, make, they, disap...</td>\n",
       "      <td>110</td>\n",
       "      <td>37</td>\n",
       "      <td>0.336364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename                                       unique_words  \\\n",
       "0  DIME002-CV.txt  [it,  is, go, play, peekaboo, this, be, we, to...   \n",
       "1  DIME002-DG.txt  [it,  is, change, do, you, want, to, play, pee...   \n",
       "2  DIME004-CV.txt  [you, be, like, a, magician, make, they, disap...   \n",
       "\n",
       "  total_number_words unique_word_count       TTR  \n",
       "0                157                29  0.184713  \n",
       "1                150                34  0.226667  \n",
       "2                110                37  0.336364  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(total_num)\n",
    "final_df.to_csv(OUTPUT_PATH_final)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d612734b-9f09-4ef3-bfc5-f3c74b5a48ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename| word| word_lemma| word_pos| pos_explain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[DIME002-CV.txt, it, it, PRON, pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[DIME002-CV.txt, 's,  is, AUX, auxiliary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[DIME002-CV.txt, gone, go, VERB, verb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[DIME002-CV.txt, play, play, VERB, verb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[DIME002-CV.txt, peekaboo, peekaboo, NOUN, noun]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename| word| word_lemma| word_pos| pos_explain\n",
       "0           [DIME002-CV.txt, it, it, PRON, pronoun]\n",
       "1         [DIME002-CV.txt, 's,  is, AUX, auxiliary]\n",
       "2            [DIME002-CV.txt, gone, go, VERB, verb]\n",
       "3          [DIME002-CV.txt, play, play, VERB, verb]\n",
       "4  [DIME002-CV.txt, peekaboo, peekaboo, NOUN, noun]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(token_pos)\n",
    "pos_df.to_csv(OUTPUT_PATH_pos)\n",
    "pos_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
